{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extracting different feature examples\n",
    "\n",
    "This sections outlines some code snippets for how you can extract some slightly different features\n",
    "The first shows how you \"resample\" the signal so that all of the signal fits into the same amount of timesteps\n",
    "by stretching or compressing them, kind of like how you can do this with an image as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv1D, MaxPool1D, Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from scipy.signal import resample  # New line\n",
    "from helpers import train_to_id5\n",
    "from helpers import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Change this to set how many steps long you want your time-series to be\n",
    "input_length = 10000\n",
    "\n",
    "\n",
    "# A function to extract the values we need as input and output for the model training\n",
    "# Note: You can make changes here to look at different features\n",
    "def extract_features(signals, train_types):\n",
    "    model_input = []\n",
    "    model_target = []\n",
    "    \n",
    "    # Iterate over all signals and corresponding train types\n",
    "    for signal, train_type in zip(signals, train_types):\n",
    "                \n",
    "        # Assemble the signal one data point\n",
    "        signal = resample(signal, input_length)\n",
    "        input_vector = np.reshape(signal, (-1, 1))  # special case if you have only 1 time series\n",
    "    \n",
    "        # Convert train type to number\n",
    "        target = train_to_id5(train_type)\n",
    "        \n",
    "        # Add to dataset to be fed to a machine learning algorithm\n",
    "        model_input.append(input_vector)\n",
    "        model_target.append(target)\n",
    "    \n",
    "    # Convert to a more digestable format and return the data, also makes also signals equally long\n",
    "    model_input = sequence.pad_sequences(model_input, input_length)\n",
    "    model_target = np.array(model_target)\n",
    "    return model_input, model_target\n",
    "\n",
    "\n",
    "# Load the data\n",
    "training_x, training_y = load_dataset(dataset='training')\n",
    "validate_x, validate_y = load_dataset(dataset='validate')\n",
    "\n",
    "# Transform the data / extract features\n",
    "training_x, training_y = extract_features(training_x, training_y)\n",
    "validate_x, validate_y = extract_features(validate_x, validate_y)\n",
    "\n",
    "\n",
    "# Build a Convolutional Neural Network\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=8, kernel_size=5, padding='valid', input_shape=training_x.shape[1:]))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Conv1D(filters=8, kernel_size=5, padding='valid'))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fit a model to the data. Note less epochs are needed here\n",
    "logger = model.fit(training_x, training_y, epochs=50, batch_size=32, validation_data=[validate_x, validate_y])\n",
    "\n",
    "# Visualize the fitting process to learn about how the model likely is\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.plot(logger.history['accuracy'])\n",
    "plt.plot(logger.history['val_accuracy'])\n",
    "plt.legend(['training accuracy', 'validation accuracy'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This following snippet shows how you can extract a \"periodogram\" from the signal. This is a type of transformation which gives you information about what kind of frequencies are present in the vibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv1D, MaxPool1D, Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from scipy.signal import periodogram  # New line\n",
    "from helpers import train_to_id5\n",
    "from helpers import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Change this to set how many steps long you want your time-series to be\n",
    "input_length = 10000\n",
    "\n",
    "\n",
    "# A function to extract the values we need as input and output for the model training\n",
    "# Note: You can make changes here to look at different features\n",
    "def extract_features(signals, train_types):\n",
    "    model_input = []\n",
    "    model_target = []\n",
    "    \n",
    "    # Needs to be done before iterating over the signals in this case\n",
    "    signals = sequence.pad_sequences(signals, input_length * 2)\n",
    "    \n",
    "    # Iterate over all signals and corresponding train types\n",
    "    for signal, train_type in zip(signals, train_types):\n",
    "                \n",
    "        # Assemble the signal one data point\n",
    "        signal = periodogram(signal)[1]\n",
    "        input_vector = np.reshape(signal, (-1, 1))  # special case if you have only 1 time series\n",
    "    \n",
    "        # Convert train type to number\n",
    "        target = train_to_id5(train_type)\n",
    "        \n",
    "        # Add to dataset to be fed to a machine learning algorithm\n",
    "        model_input.append(input_vector)\n",
    "        model_target.append(target)\n",
    "    \n",
    "    # Convert to a more digestable format and return the data, also makes also signals equally long\n",
    "    model_input = np.array(model_input)\n",
    "    model_target = np.array(model_target)\n",
    "    return model_input, model_target\n",
    "\n",
    "\n",
    "# Load the data\n",
    "training_x, training_y = load_dataset(dataset='training')\n",
    "validate_x, validate_y = load_dataset(dataset='validate')\n",
    "\n",
    "# Transform the data / extract features\n",
    "training_x, training_y = extract_features(training_x, training_y)\n",
    "validate_x, validate_y = extract_features(validate_x, validate_y)\n",
    "\n",
    "\n",
    "# Build a Convolutional Neural Network\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=8, kernel_size=5, padding='valid', input_shape=training_x.shape[1:]))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Conv1D(filters=8, kernel_size=5, padding='valid'))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fit a model to the data. Note less epochs are needed here\n",
    "logger = model.fit(training_x, training_y, epochs=50, batch_size=32, validation_data=[validate_x, validate_y])\n",
    "\n",
    "# Visualize the fitting process to learn about how the model likely is\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.plot(logger.history['accuracy'])\n",
    "plt.plot(logger.history['val_accuracy'])\n",
    "plt.legend(['training accuracy', 'validation accuracy'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from helpers import train_to_id5\n",
    "from helpers import load_dataset\n",
    "\n",
    "\n",
    "# A function to extract the values we need as input and output for the model training\n",
    "# Note: You can make changes here to look at different features\n",
    "def extract_features(signals, train_types):\n",
    "    model_input = []\n",
    "    model_target = []\n",
    "    \n",
    "    # Iterate over all signals and corresponding train types\n",
    "    for signal, train_type in zip(signals, train_types):\n",
    "        \n",
    "        # Extract signal features with suggestion for alternative features\n",
    "        rms_sub = np.sqrt(np.mean(np.square(signal[2000:6000])))  # Only extracts rms values for a subset of the signal\n",
    "        signal_mean = np.mean(signal)\n",
    "        signal_abs_mean = np.mean(np.abs(signal))\n",
    "        signal_median = np.median(signal)\n",
    "        signal_abs_median = np.median(np.abs(signal))\n",
    "        quantile_25 = np.percentile(signal, 25)\n",
    "        quantile_75 = np.percentile(signal, 75)\n",
    "        length = len(signal)\n",
    "        \n",
    "        # Assemble these values into a single data point / array\n",
    "        # You can combine any or all of the above in any way you want\n",
    "        input_vector = [rms_sub, signal_mean, signal_abs_mean, length]\n",
    "    \n",
    "        # Convert train type to number\n",
    "        target = train_to_id5(train_type)\n",
    "        \n",
    "        # Add to dataset to be fed to a machine learning algorithm\n",
    "        model_input.append(input_vector)\n",
    "        model_target.append(target)\n",
    "    \n",
    "    # Convert to a more digestable format and return the data\n",
    "    model_input = np.array(model_input)\n",
    "    model_target = np.array(model_target)\n",
    "    return model_input, model_target\n",
    "\n",
    "\n",
    "# Load the data\n",
    "training_x, training_y = load_dataset(dataset='training')\n",
    "validate_x, validate_y = load_dataset(dataset='validate')\n",
    "\n",
    "# Transform the data / extract features\n",
    "training_x, training_y = extract_features(training_x, training_y)\n",
    "validate_x, validate_y = extract_features(validate_x, validate_y)\n",
    "\n",
    "# Build a simple Neural Network\n",
    "model = Sequential()\n",
    "model.add(Dense(units=5, input_dim=training_x.shape[1]))\n",
    "model.add(Dense(units=10))\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Apply the data and the train types and have the algorithm fit a model from x to y\n",
    "logger = model.fit(training_x, training_y, epochs=250, batch_size=32, validation_data=[validate_x, validate_y])\n",
    "\n",
    "# You can add a filter after getting the model predictions to augment what it does and then validate the results\n",
    "correct = 0\n",
    "predicted_y = model.predict(validate_x)\n",
    "for i in range(len(predicted_y)):\n",
    "    # The below if statement checks if the i-th signal had a length shorter than 50000 timesteps, assuming no changes\n",
    "    # were made above. keep index number in mind for easy access and additional checks if you find good filters\n",
    "    if validate_x[i][3] < 50000:\n",
    "        if np.argmax(predicted_y[i]) == np.argmax(validate_y[i]):  # Compares model output with model target\n",
    "            correct += 1\n",
    "    else:\n",
    "        if np.sum(validate_y[i]) == 0:  # Verifies that the model target is also an unknown train type\n",
    "            correct += 1\n",
    "# Validation accuracy after filtering\n",
    "print('True validation accuracy: %.2f%%' % (100.0 * float(correct) / float(len(predicted_y))))\n",
    "\n",
    "\n",
    "# Visualize the fitting process to learn about how good the model likely is\n",
    "# Note, validation accuracy is the metric of how good the model probably is, while training accuracy shows\n",
    "# how quickly the algorithm found a way to map the input to the desired output\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.plot(logger.history['accuracy'])\n",
    "plt.plot(logger.history['val_accuracy'])\n",
    "plt.legend(['training accuracy', 'validation accuracy'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Changing the model target\n",
    "\n",
    "Until now, we have been setting \"unknown\" trains as it's own train type which we try to classify. We can however also try to only classify the 4 relevant types of trains and treating the absence of this as an \"unknown\" type.\n",
    "This can be achieved by using the helper function \"train_to_id4\" and changing a couple of things in the model, in particular we'll use \"binary_crossentropy\" instead of \"categorical_crossentropy\", which means we try to determine how likely it is that this signal is that particular train type, meaning it could theoretically possible for it to think that it could be 2 types of trains at the same time. This also changes the output of the validation accuracy a bit since it now check for accuracy over 4 outputs instead of one per sample, i.e. if 3 of the 4 output values say it is not this train type and the last one say it is this one, even if it got the train type wrong it would still be 50% correct (2 correct \"not this type\" and 1 incorrect \"not this type\" as well as 1 incorrect \"it is this type\"). Feel free to ask me for more details about this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv1D, MaxPool1D, Flatten\n",
    "from keras.models import Sequential\n",
    "from helpers import train_to_id4\n",
    "from helpers import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Change this to set how many steps long you want your time-series to be\n",
    "input_length = 10000\n",
    "\n",
    "\n",
    "# A function to extract the values we need as input and output for the model training\n",
    "# Note: You can make changes here to look at different features\n",
    "def extract_features(signals, train_types):\n",
    "    model_input = []\n",
    "    model_target = []\n",
    "    \n",
    "    # Iterate over all signals and corresponding train types\n",
    "    for signal, train_type in zip(signals, train_types):\n",
    "                \n",
    "        # Assemble the signal one data point\n",
    "        input_vector = np.reshape(signal, (-1, 1))  # special case if you have only 1 time series\n",
    "    \n",
    "        # Convert train type to number\n",
    "        target = train_to_id4(train_type)\n",
    "        \n",
    "        # Add to dataset to be fed to a machine learning algorithm\n",
    "        model_input.append(input_vector)\n",
    "        model_target.append(target)\n",
    "    \n",
    "    # Convert to a more digestable format and return the data, also makes also signals equally long\n",
    "    model_input = np.array(model_input)\n",
    "    model_target = np.array(model_target)\n",
    "    return model_input, model_target\n",
    "\n",
    "\n",
    "# Load the data\n",
    "training_x, training_y = load_dataset(dataset='training')\n",
    "validate_x, validate_y = load_dataset(dataset='validate')\n",
    "\n",
    "# Transform the data / extract features\n",
    "training_x, training_y = extract_features(training_x, training_y)\n",
    "validate_x, validate_y = extract_features(validate_x, validate_y)\n",
    "\n",
    "\n",
    "# Build a Convolutional Neural Network\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=8, kernel_size=5, padding='valid', input_shape=training_x.shape[1:]))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Conv1D(filters=8, kernel_size=5, padding='valid'))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=4, activation='sigmoid'))  # softmax does not work well with binary_crossentropy\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fit a model to the data. Note less epochs are needed here\n",
    "logger = model.fit(training_x, training_y, epochs=50, batch_size=32, validation_data=[validate_x, validate_y])\n",
    "\n",
    "# Some more code needs to be used to determine the true validation accuracy when using this type of model output\n",
    "correct = 0\n",
    "predicted_y = model.predict(validate_x)\n",
    "for i in range(len(predicted_y)):\n",
    "    if np.max(predicted_y[i]) >= 0.50:  # This is a threshold to determine if any train types were identified\n",
    "        if np.argmax(predicted_y[i]) == np.argmax(validate_y[i]):  # Compares model output with model target\n",
    "            correct += 1\n",
    "    else:\n",
    "        if np.sum(validate_y[i]) == 0:  # Verifies that the model target is also an unknown train type\n",
    "            correct += 1\n",
    "print('True validation accuracy: %.2f%%' % (100.0 * float(correct) / float(len(predicted_y))))\n",
    "\n",
    "# Visualize the fitting process to learn about how the model likely is\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.plot(logger.history['accuracy'])\n",
    "plt.plot(logger.history['val_accuracy'])\n",
    "plt.legend(['training accuracy', 'validation accuracy'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
