{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example 1\n",
    "\n",
    "This example builds on the feature extraction from the workshop notebook, and builds upon it with some alternative feature extraction methods. Note it is _not_ required for you to fully understand what is going on with each of these methods. These can all be found in the \"extract_feature\" function. Quick explanation of each\n",
    "- resample: stretch or compress signal such that the whole signal fits into the chosen input_length\n",
    "- Min-Max Normalize: takes the biggest value and scales the signal so that all the values are between [-1, 1]. This can be helpful for Machine Learning algorithms, as they can get unstable if the values it tries to approximate have to big a difference\n",
    "- log normalize: This is a technique which give more importantce to thw lower values and less importance to high values. The job here is also to make it easier for the Machine Learning algorithm to find a good model.\n",
    "- welch: this is a method computing a PSD (Power Spectral Density). This is an estimate of what frequencies are present in the signal. Note that for this method, input_length _must_ be shorter than the shortest signal for it to not to crash (i.e. less than 7000 here)\n",
    "\n",
    "### Exercise:\n",
    "Using which features give you the highest validation score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import train_to_id5, load_dataset, plot_validation_history\n",
    "from scipy.signal import welch, resample\n",
    "from keras.layers import Dense, Conv1D, MaxPool1D, Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Change this to set how many steps long you want your time-series to be\n",
    "input_length = 5000\n",
    "\n",
    "\n",
    "# A function to extract the values we need as input and output for the model training\n",
    "# Note: You can make changes here to look at different features\n",
    "def extract_features(signals, train_types):\n",
    "    model_input = []\n",
    "    model_target = []\n",
    "    \n",
    "    # Iterate over all signals and corresponding train types\n",
    "    for signal, train_type in zip(signals, train_types):\n",
    "                \n",
    "        # Assemble the signal one data point\n",
    "        # ------- You can uncomment any single line below or combine some -------- #\n",
    "        # signal = welch(signal, nperseg=input_length * 2)[1][1:]  # Compute an estimate of the Power Spectrum Density\n",
    "        # signal = resample(signal, input_length)  # Stretch / Compress signal to fit\n",
    "        # signal = signal / np.max(np.abs(signal))  # Min-Max Normalize\n",
    "        # signal = np.sign(signal) * np.log(signal + 1.0)  # Reduce the range of the signal\n",
    "        input_vector = np.reshape(signal, (-1, 1))  # special case if you have only 1 time series\n",
    "    \n",
    "        # Convert train type to number\n",
    "        target = train_to_id5(train_type)\n",
    "        \n",
    "        # Add to dataset to be fed to a machine learning algorithm\n",
    "        model_input.append(input_vector)\n",
    "        model_target.append(target)\n",
    "    \n",
    "    # Convert to a more digestable format and return the data, also makes also signals equally long\n",
    "    model_input = sequence.pad_sequences(model_input, input_length)\n",
    "    model_target = np.array(model_target)\n",
    "    return model_input, model_target\n",
    "\n",
    "\n",
    "# Load the data\n",
    "training_x, training_y = load_dataset(dataset='training')\n",
    "validate_x, validate_y = load_dataset(dataset='validate')\n",
    "\n",
    "# Transform the data / extract features\n",
    "training_x, training_y = extract_features(training_x, training_y)\n",
    "validate_x, validate_y = extract_features(validate_x, validate_y)\n",
    "\n",
    "# Build a Convolutional Neural Network\n",
    "# ------- You can change the number of filters and kernel_size here --------- #\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=4, kernel_size=5, padding='valid', input_shape=training_x.shape[1:]))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Conv1D(filters=4, kernel_size=5, padding='valid'))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit a model to the data. Note less epochs are needed here\n",
    "# ------- You can change the number of epochs and batch_size here ----------- #\n",
    "logger = model.fit(training_x, training_y, epochs=25, batch_size=16, validation_data=[validate_x, validate_y])\n",
    "plot_validation_history(logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example 2\n",
    "\n",
    "It is also possible to simply extract a couple of numbers from the signal, like the maximum value, the length of the signal and a number of other features. Then this could be fed into a simpler and tinier type of neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from helpers import train_to_id5\n",
    "from helpers import load_dataset\n",
    "\n",
    "\n",
    "# A function to extract the values we need as input and output for the model training\n",
    "# Note: You can make changes here to look at different features\n",
    "def extract_features(signals, train_types):\n",
    "    model_input = []\n",
    "    model_target = []\n",
    "    \n",
    "    # Iterate over all signals and corresponding train types\n",
    "    for signal, train_type in zip(signals, train_types):\n",
    "        \n",
    "        # Extract signal features with suggestion for alternative features\n",
    "        signal_rms = np.sqrt(np.mean(np.square(signal)))  # Root Mean Square of the signal\n",
    "        signal_mean = np.mean(signal)  # The mean value\n",
    "        signal_abs_mean = np.mean(np.abs(signal))  # The mean value when all negative values are turned positive\n",
    "        signal_median = np.median(signal)  # The median value\n",
    "        signal_abs_median = np.median(np.abs(signal))  # The median value when all negative values are turned positive\n",
    "        percentile_25 = np.percentile(signal, 25)  # 25 percentile value\n",
    "        percentile_75 = np.percentile(signal, 75)  # 75 percentile value\n",
    "        length = len(signal)  # number of timesteps in the signal\n",
    "        \n",
    "        # Assemble these values into a single data point / array\n",
    "        # You can combine any or all of the above in any way you want\n",
    "        input_vector = [signal_rms, signal_mean, signal_abs_mean, length]\n",
    "    \n",
    "        # Convert train type to number\n",
    "        target = train_to_id5(train_type)\n",
    "        \n",
    "        # Add to dataset to be fed to a machine learning algorithm\n",
    "        model_input.append(input_vector)\n",
    "        model_target.append(target)\n",
    "    \n",
    "    # Convert to a more digestable format and return the data\n",
    "    model_input = np.array(model_input)\n",
    "    model_target = np.array(model_target)\n",
    "    return model_input, model_target\n",
    "\n",
    "\n",
    "# Load the data\n",
    "training_x, training_y = load_dataset(dataset='training')\n",
    "validate_x, validate_y = load_dataset(dataset='validate')\n",
    "\n",
    "# Transform the data / extract features\n",
    "training_x, training_y = extract_features(training_x, training_y)\n",
    "validate_x, validate_y = extract_features(validate_x, validate_y)\n",
    "\n",
    "# Build a simple Neural Network\n",
    "model = Sequential()\n",
    "model.add(Dense(units=5, input_dim=training_x.shape[1]))\n",
    "model.add(Dense(units=10))\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Apply the data and the train types and have the algorithm fit a model from x to y\n",
    "logger = model.fit(training_x, training_y, epochs=250, batch_size=32, validation_data=[validate_x, validate_y])\n",
    "\n",
    "# You can add a filter after getting the model predictions to augment what it does and then validate the results\n",
    "correct = 0\n",
    "predicted_y = model.predict(validate_x)\n",
    "for i in range(len(predicted_y)):\n",
    "    # The below if statement checks if the i-th signal had a length shorter than 50000 timesteps, assuming no changes\n",
    "    # were made above. keep index number in mind for easy access and additional checks if you find good filters\n",
    "    # ----- You can change the value here to improve the results ----- #\n",
    "    if validate_x[i][3] < 50000:\n",
    "        if np.argmax(predicted_y[i]) == np.argmax(validate_y[i]):  # Compares model output with model target\n",
    "            correct += 1\n",
    "    else:\n",
    "        if np.sum(validate_y[i]) == 0:  # Verifies that the model target is also an unknown train type\n",
    "            correct += 1\n",
    "\n",
    "# Validation accuracy after filtering\n",
    "accuracy = float(correct) / float(len(predicted_y))\n",
    "plot_validation_history(logger, accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example 3\n",
    "\n",
    "Another thing we can try is to change the model target. Instead of treating \"unknown\" trains as its own train type, we can treat it as the absense of a train type. The way we do this is to change the function which does the 1-hot encoding to do this:\n",
    "- train_to_id5('train_b') = [0, 1, 0, 0, 0]\n",
    "- train_to_id4('train_b') = [0, 1, 0, 0]\n",
    "\n",
    "This means we need to change a couple of other things as well, in particular we will use a different \"loss-function\", which is what the neural network uses to get errors to correct for. \"categorical_crossentropy\" uses the output node with the highest value, and checks if this is the same as the expected target. If the network returns [0.2, 0.1, 0.4, 0.1, 0.2] and the target is [0, 0, 1, 0, 0] ('train_c'), it will treat this as 1 correct prediction. This is only useful if the data you present only ever has 1 class in them. \"binary_crossentropy\" assumes that all elements can be either 0 or 1. Anything above 0.50 will be trated as if it was 1, meaning it will treat the model output [0.0, 0.2, 0.6, 0.3] where the expected target is [0, 0, 0, 1] ('train_d'), it will treat this as 50% correct, since the first two entries are correct, and the last two are incorrect.\n",
    "\n",
    "### Exercises:\n",
    "- Find the one change required to make the script run correctly when you use target_to_id4 instead of target_to_id5\n",
    "- Does this increase the real validation accuracy?\n",
    "- Can you think of any other reason for using this alternative method? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from helpers import train_to_id4, train_to_id5, load_dataset, plot_validation_history\n",
    "from scipy.signal import welch, resample\n",
    "from keras.layers import Dense, Conv1D, MaxPool1D, Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Change this to set how many steps long you want your time-series to be\n",
    "input_length = 5000\n",
    "\n",
    "\n",
    "# A function to extract the values we need as input and output for the model training\n",
    "# Note: You can make changes here to look at different features\n",
    "def extract_features(signals, train_types):\n",
    "    model_input = []\n",
    "    model_target = []\n",
    "    \n",
    "    # Iterate over all signals and corresponding train types\n",
    "    for signal, train_type in zip(signals, train_types):\n",
    "                \n",
    "        # Assemble the signal one data point\n",
    "        # ------- You can uncomment any single line below or combine some -------- #\n",
    "        # signal = welch(signal, nperseg=input_length * 2)[1][1:]  # Compute an estimate of the Power Spectrum Density\n",
    "        # signal = resample(signal, input_length)  # Stretch / Compress signal to fit\n",
    "        # signal = signal / np.max(np.abs(signal))  # Min-Max Normalize\n",
    "        # signal = np.sign(signal) * np.log(signal + 1.0)  # Reduce the range of the signal\n",
    "        input_vector = np.reshape(signal, (-1, 1))  # special case if you have only 1 time series\n",
    "    \n",
    "        # Convert train type to number\n",
    "        target = train_to_id5(train_type)  # Old method, returns array of 5 elements\n",
    "        # target = train_to_id4(train_type)  # New method, returns array of 4 elements\n",
    "        \n",
    "        # Add to dataset to be fed to a machine learning algorithm\n",
    "        model_input.append(input_vector)\n",
    "        model_target.append(target)\n",
    "    \n",
    "    # Convert to a more digestable format and return the data, also makes also signals equally long\n",
    "    model_input = sequence.pad_sequences(model_input, input_length)\n",
    "    model_target = np.array(model_target)\n",
    "    return model_input, model_target\n",
    "\n",
    "\n",
    "# Load the data\n",
    "training_x, training_y = load_dataset(dataset='training')\n",
    "validate_x, validate_y = load_dataset(dataset='validate')\n",
    "\n",
    "# Transform the data / extract features\n",
    "training_x, training_y = extract_features(training_x, training_y)\n",
    "validate_x, validate_y = extract_features(validate_x, validate_y)\n",
    "\n",
    "# Build a Convolutional Neural Network\n",
    "# ------- You can change the number of filters and kernel_size here --------- #\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=4, kernel_size=5, padding='valid', input_shape=training_x.shape[1:]))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Conv1D(filters=4, kernel_size=5, padding='valid'))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=5, activation='sigmoid'))\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit a model to the data. Note less epochs are needed here\n",
    "# ------- You can change the number of epochs and batch_size here ----------- #\n",
    "logger = model.fit(training_x, training_y, epochs=25, batch_size=16, validation_data=[validate_x, validate_y])\n",
    "\n",
    "# Some more code needs to be used to determine the true validation accuracy when using this type of model output\n",
    "correct = 0\n",
    "true_positive, false_positive, true_negative, false_negative = 0, 0, 0, 0\n",
    "predicted_y = model.predict(validate_x)\n",
    "for i in range(len(predicted_y)):\n",
    "    if np.max(predicted_y[i]) >= 0.50:  # This is a threshold to determine if any train types were identified\n",
    "        if np.argmax(predicted_y[i]) == np.argmax(validate_y[i]):  # Compares model output with model target\n",
    "            correct += 1\n",
    "            true_positive += 1\n",
    "        else:\n",
    "            false_positive += 1\n",
    "    else:\n",
    "        if np.sum(validate_y[i]) == 0:  # Verifies that the model target is also an unknown train type\n",
    "            correct += 1\n",
    "            true_negative += 1\n",
    "        else:\n",
    "            false_negative += 1\n",
    "accuracy = float(correct) / float(len(predicted_y))\n",
    "plot_validation_history(logger, accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
