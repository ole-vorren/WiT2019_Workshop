{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Workshop Notebook\n",
    "\n",
    "This jupyter notebook will make for the interactive part of this workshop\n",
    "\n",
    "## Step 1: Inspect the data\n",
    "\n",
    "Usually, the first thing we want to do when dealing with any new type of data, we want to inspect it first to get some intuitions for it. By visualizing the data, we can often get some ideas as to how to tackle the data and features we can extract from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "label_file = 'data/training_labels.csv'\n",
    "df = pd.read_csv(label_file)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What has been done here, is to load a csv file containing rows of filepaths and correspendong train types. The filepaths are stored as binary blobs which can be found in data/signals. The table shown above is an excerpt of this list as it has been read into a dataframe\n",
    "Let us explore a couple of the signatures we can find there. I also encourage you to look at more of them to get an even better idea of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from helpers import load_binary\n",
    "from helpers import plot_size\n",
    "%matplotlib inline\n",
    "\n",
    "type_a = df.loc[df['train_type'] == 'train_a']\n",
    "type_b = df.loc[df['train_type'] == 'train_b']\n",
    "type_c = df.loc[df['train_type'] == 'train_c']\n",
    "type_d = df.loc[df['train_type'] == 'train_d']\n",
    "\n",
    "file_a = 'data/signals/training/' + type_a['filename'].iloc[0]\n",
    "file_b = 'data/signals/training/' + type_b['filename'].iloc[0]\n",
    "file_c = 'data/signals/training/' + type_c['filename'].iloc[0]\n",
    "file_d = 'data/signals/training/' + type_d['filename'].iloc[0]\n",
    "\n",
    "signal_a = load_binary(file_a)\n",
    "signal_b = load_binary(file_b)\n",
    "signal_c = load_binary(file_c)\n",
    "signal_d = load_binary(file_d)\n",
    "\n",
    "plot_size(16, 8)\n",
    "plt.subplot(411)\n",
    "plt.title('Train A')\n",
    "plt.plot(signal_a)\n",
    "plt.subplot(412)\n",
    "plt.title('Train B')\n",
    "plt.plot(signal_b)\n",
    "plt.subplot(413)\n",
    "plt.title('Train C')\n",
    "plt.plot(signal_c)\n",
    "plt.subplot(414)\n",
    "plt.title('Train D')\n",
    "plt.plot(signal_d)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can already see a couple of things to expect from the data. The data comes in timeseries with thousands of timesteps and come with variable lengths. It's filled with impulses, likely from when wheels of the train is passing over the sensor. This means a couples of things already:\n",
    "1. The data will not go well with a lot of Machine Learning methods as is, as there are too many timesteps in the signal\n",
    "2. When trying to fit a model, we would want to consider some way of making the signals the same size\n",
    "\n",
    "To get a better idea of the ranges of the data, we ought to compute some aggregate statistics as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Allocate a bit of space for storing data\n",
    "lengths = []\n",
    "rms_values = []\n",
    "value_max = []\n",
    "\n",
    "# Iterate over all the signals and collect data from them\n",
    "for i in range(len(df)):\n",
    "    filename = df['filename'][i]\n",
    "    train_type = df['filename'][i]\n",
    "    \n",
    "    # uncomment the next 2 lines to filter for specific train types\n",
    "    # if train_type != 'train_a':\n",
    "    #     continue\n",
    "    \n",
    "    data = load_binary('data/signals/training/' + filename)\n",
    "    rms = np.sqrt(np.mean(np.square(data)))  # Compute the Root Mean Square\n",
    "    length = len(data)\n",
    "    \n",
    "    # Append values to lists\n",
    "    lengths.append(length)\n",
    "    rms_values.append(rms)\n",
    "    value_max.append(max(np.abs(data)))\n",
    "\n",
    "# Plot histograms and a bar plot of the gathered data to determine the spread of values\n",
    "plot_size(16, 8)\n",
    "plt.subplot(141)\n",
    "plt.title('Signal Lengths')\n",
    "plt.hist(lengths)\n",
    "plt.subplot(142)\n",
    "plt.title('Signal RMS')\n",
    "plt.hist(rms_values)\n",
    "plt.subplot(143)\n",
    "plt.title('Signal Maximas')\n",
    "plt.hist(value_max)\n",
    "plt.subplot(144)\n",
    "plt.title('Train Type Distribution')\n",
    "df['train_type'].value_counts().plot(kind='bar')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "From this, we can see that most of the signals are around 20000 timesteps long or shorter. We also see that there is quite a range of both maximum values of RMS values, but with stronger tendencies towards the lower end of the spectrum. We also see that we have a relatively even spread of train types, and also a category called \"unknown\". These are signals for which the train type is not known. The reason we look at this is to determine a number of things\n",
    "1. See if there are strong tendencies in the data for which we can make simple filters\n",
    "2. To help determine how data needs to be transformed for a potential model\n",
    "3. Determine if special weighting or other techniques are required to deal with imbalanced datasets\n",
    "\n",
    "Questions:\n",
    "- What would you look at next with basis in this data?\n",
    "- What is the simplest model you could use as a baseline for determining the train type here?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 2: Simple approaches to modelling the data\n",
    "\n",
    "Let's start with a couple of simple ways to model the data.\n",
    "There's a couple of ways we can go about it.\n",
    "- We can try to extract simple features from the signal to work with, and apply it to a modelling technique of our choice\n",
    "- We can try to use the signals themselves as features within a modelling architecture\n",
    "\n",
    "For simplicity, and time reasons we'll be building Neural Networks using a library called \"Keras\". If you're familiar with scikit-learn, feel free to use it to model data as well. Let's start with a very simple model using the 3 features from before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from helpers import train_to_id5\n",
    "from helpers import load_dataset\n",
    "\n",
    "\n",
    "# A function to extract the values we need as input and output for the model training\n",
    "# Note: You can make changes here to look at different features\n",
    "def extract_features(signals, train_types):\n",
    "    model_input = []\n",
    "    model_target = []\n",
    "    \n",
    "    # Iterate over all signals and corresponding train types\n",
    "    for signal, train_type in zip(signals, train_types):\n",
    "        \n",
    "        # Extract signal features\n",
    "        rms = np.sqrt(np.mean(np.square(signal)))\n",
    "        max_value = np.max(np.abs(signal))\n",
    "        length = len(signal) / 1000.0  # Lower this value to a similar output as the other features\n",
    "        \n",
    "        # Assemble these values into a single data point / array\n",
    "        input_vector = [rms, max_value, length]\n",
    "    \n",
    "        # Convert train type to number\n",
    "        target = train_to_id5(train_type)\n",
    "        \n",
    "        # Add to dataset to be fed to a machine learning algorithm\n",
    "        model_input.append(input_vector)\n",
    "        model_target.append(target)\n",
    "    \n",
    "    # Convert to a more digestable format and return the data\n",
    "    model_input = np.array(model_input)\n",
    "    model_target = np.array(model_target)\n",
    "    return model_input, model_target\n",
    "\n",
    "\n",
    "# Load the data\n",
    "training_x, training_y = load_dataset(dataset='training')\n",
    "validate_x, validate_y = load_dataset(dataset='validate')\n",
    "\n",
    "# Transform the data / extract features\n",
    "training_x, training_y = extract_features(training_x, training_y)\n",
    "validate_x, validate_y = extract_features(validate_x, validate_y)\n",
    "\n",
    "# Build a simple Neural Network\n",
    "model = Sequential()\n",
    "model.add(Dense(units=5, input_dim=training_x.shape[1]))\n",
    "model.add(Dense(units=10))\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Apply the data and the train types and have the algorithm fit a model from x to y\n",
    "# Quick primer on fitting:\n",
    "# epochs = number of iterations spent trying to fit the model output to the target output\n",
    "# validation_data = data not used for fitting the model. Used to determine how well the model works on unseen data\n",
    "# batch_size = number of samples to update over in one step of the optimizing algorithm.\n",
    "#   with batch_size, usually more means faster fitting time and more stable results, but worse generalization\n",
    "logger = model.fit(training_x, training_y, epochs=250, batch_size=32, validation_data=[validate_x, validate_y])\n",
    "\n",
    "# Visualize the fitting process to learn about how good the model likely is\n",
    "# Note, validation accuracy is the metric of how good the model probably is, while training accuracy shows\n",
    "# how quickly the algorithm found a way to map the input to the desired output\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.plot(logger.history['accuracy'])\n",
    "plt.plot(logger.history['val_accuracy'])\n",
    "plt.legend(['training accuracy', 'validation accuracy'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Okay, we have improved on the baseline by a bit, but can we do better? Let us try to extract features from the signal directly. Note that in order to do so, however, we need to work with the signal a little bit first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv1D, MaxPool1D, Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from helpers import train_to_id5\n",
    "\n",
    "\n",
    "# Change this to set how many steps long you want your time-series to be\n",
    "input_length = 10000\n",
    "\n",
    "\n",
    "# A function to extract the values we need as input and output for the model training\n",
    "# Note: You can make changes here to look at different features\n",
    "def extract_features(signals, train_types):\n",
    "    model_input = []\n",
    "    model_target = []\n",
    "    \n",
    "    # Iterate over all signals and corresponding train types\n",
    "    for signal, train_type in zip(signals, train_types):\n",
    "                \n",
    "        # Assemble the signal one data point\n",
    "        input_vector = np.reshape(signal, (-1, 1))  # special case if you have only 1 time series\n",
    "        # input_vector = [signal, signal]  # uncomment to add multiple equally long time series to your model input\n",
    "    \n",
    "        # Convert train type to number\n",
    "        target = train_to_id5(train_type)\n",
    "        \n",
    "        # Add to dataset to be fed to a machine learning algorithm\n",
    "        model_input.append(input_vector)\n",
    "        model_target.append(target)\n",
    "    \n",
    "    # Convert to a more digestable format and return the data, also makes also signals equally long\n",
    "    model_input = sequence.pad_sequences(model_input, input_length)\n",
    "    model_target = np.array(model_target)\n",
    "    return model_input, model_target\n",
    "\n",
    "\n",
    "# Load the data\n",
    "training_x, training_y = load_dataset(dataset='training')\n",
    "validate_x, validate_y = load_dataset(dataset='validate')\n",
    "\n",
    "# Transform the data / extract features\n",
    "training_x, training_y = extract_features(training_x, training_y)\n",
    "validate_x, validate_y = extract_features(validate_x, validate_y)\n",
    "\n",
    "\n",
    "# Build a Convolutional Neural Network\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=8, kernel_size=5, padding='valid', input_shape=training_x.shape[1:]))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Conv1D(filters=8, kernel_size=5, padding='valid'))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fit a model to the data. Note less epochs are needed here\n",
    "logger = model.fit(training_x, training_y, epochs=25, batch_size=32, validation_data=[validate_x, validate_y])\n",
    "\n",
    "# Visualize the fitting process to learn about how the model likely is\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.plot(logger.history['accuracy'])\n",
    "plt.plot(logger.history['val_accuracy'])\n",
    "plt.legend(['training accuracy', 'validation accuracy'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now it is your turn. How good a model can you build for determining the correct train types? Some things to explore\n",
    "- Can you extract some better features from the signal to build a better model?\n",
    "- Does tweaking with the model parameters help the output? (change the amount of units, the amount of layers, etc.)\n",
    "- What is the most important qualities for the output of the model? Are there better ways to achieve this?\n",
    "- Maybe combining multiple streams of data gives better results\n",
    "\n",
    "In the \"examples\" notebook you will find code to help you do different things suggested. You can copy paste them into the snippets above or below, or you could also make an entirely new code segment combining different things. Your goal is to try to achieve as high stable results on the validation data as possible. To do this, you will need to run the model a couple of times since there is a bit of randomness in these models which lead to different results each time.\n",
    "Try to not use too high numbers in the model parameters however, as the training time will end up being far too long for this session if you do.\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to paste code into here if you want to keep the code above clean and copy-pastable\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
