{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Workshop Notebook\n",
    "\n",
    "This jupyter notebook will make for the interactive part of this workshop\n",
    "\n",
    "## Step 1: Inspect the data\n",
    "\n",
    "Usually, the first thing we want to do when dealing with any new type of data, we want to inspect it first to get some intuitions for it. By visualizing the data, we can often get some ideas as to how to tackle the data and features we can extract from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Hello World! I am ready to learn!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 2: Explore the data\n",
    "\n",
    "The first thing a Data Scientist wants to do when approaching a data problem, is to first have a look at the data. As we are looking at vibration signals, we should take a look at what these look like.\n",
    "\n",
    "\n",
    "\n",
    "What has been done here, is to load a csv file containing rows of filepaths and correspendong train types. The filepaths are stored as binary blobs which can be found in data/signals. The table shown above is an excerpt of this list as it has been read into a dataframe\n",
    "Let us explore a couple of the signatures we can find there. I also encourage you to look at more of them to get an even better idea of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from helpers import load_binary\n",
    "from helpers import plot_size\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('data/training_labels.csv')\n",
    "type_a = df.loc[df['train_type'] == 'train_a']\n",
    "type_b = df.loc[df['train_type'] == 'train_b']\n",
    "type_c = df.loc[df['train_type'] == 'train_c']\n",
    "type_d = df.loc[df['train_type'] == 'train_d']\n",
    "type_u = df.loc[df['train_type'] == 'unknown']\n",
    "\n",
    "file_a = 'data/signals/training/' + type_a['filename'].iloc[0]\n",
    "file_b = 'data/signals/training/' + type_b['filename'].iloc[0]\n",
    "file_c = 'data/signals/training/' + type_c['filename'].iloc[0]\n",
    "file_d = 'data/signals/training/' + type_d['filename'].iloc[0]\n",
    "file_u = 'data/signals/training/' + type_u['filename'].iloc[0]\n",
    "\n",
    "signal_a = load_binary(file_a)\n",
    "signal_b = load_binary(file_b)\n",
    "signal_c = load_binary(file_c)\n",
    "signal_d = load_binary(file_d)\n",
    "signal_u = load_binary(file_u)\n",
    "\n",
    "plot_size(16, 8)\n",
    "plt.subplot(511)\n",
    "plt.title('Train A')\n",
    "plt.plot(signal_a)\n",
    "plt.subplot(512)\n",
    "plt.title('Train B')\n",
    "plt.plot(signal_b)\n",
    "plt.subplot(513)\n",
    "plt.title('Train C')\n",
    "plt.plot(signal_c)\n",
    "plt.subplot(514)\n",
    "plt.title('Train D')\n",
    "plt.plot(signal_d)\n",
    "plt.subplot(515)\n",
    "plt.title('Unknown Train')\n",
    "plt.plot(signal_d)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can already see a couple of things to expect from the data. The data comes in timeseries with thousands of timesteps and come with variable lengths. It's filled with impulses, likely from when wheels of the train is passing over the sensor. It is generally hard, and time-consuming for people to find good patterns to extract manually from long signals like these, so it would be nice if a Machine Learning Algorithm could learn something by itself which correlates with the train types we want to recognize\n",
    "\n",
    "## Step 3: Simple Neural Network\n",
    "\n",
    "Neural networks generally wants to take one type of input to produce one type of output. This means that we need to do something about the fact that the different signals have different lengths. There's a couple of intuitive ways to do this:\n",
    "<img src=\"files/images/timeseries_feature.png\">\n",
    "We can either cut and pad the signal after a certain point, or we can stretch / compress the signals to make them all equally long. The shorter the signal, the easier and faster we will be able to train neural networks to find good patterns, but it also possible that there will not be enough information left in the signal to get the best results.\n",
    "\n",
    "Another thing, is that neural networks do not understand words, which means we must change our \"target variables\" to numbers. What we do here is to use something called one-hot encoding which means that we have an array with as many elements as there are classes to classify, and we represent each index with one class, thus\n",
    "- \"train_a\" -> [1, 0, 0, 0, 0]\n",
    "- \"train_b\" -> [0, 1, 0, 0, 0]\n",
    "- \"train_c\" -> [0, 0, 1, 0, 0]\n",
    "- \"train_d\" -> [0, 0, 0, 1, 0]\n",
    "- \"unknown\" -> [0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from helpers import train_to_id5, load_dataset, plot_validation_history\n",
    "from keras.layers import Dense, Conv1D, MaxPool1D, Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from scipy.signal import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Change this to set how many steps long you want your time-series to be\n",
    "# ---- You can modify this number to decide how long signals you want to use ---- #\n",
    "input_length = 5000\n",
    "\n",
    "\n",
    "# A function to extract the values we need as input and output for the model training\n",
    "# -------- You can make changes in this function -------- #\n",
    "def extract_features(signals, train_types):\n",
    "    model_input = []\n",
    "    model_target = []\n",
    "    \n",
    "    # Iterate over all signals and corresponding train types\n",
    "    for signal, train_type in zip(signals, train_types):\n",
    "                \n",
    "        # Assemble the signal one data point\n",
    "        # --------- Uncomment Line below to stretch / compress signal ----------- #\n",
    "        # signal = resample(signal, input_length)\n",
    "        input_vector = np.reshape(signal, (-1, 1))  # special case if you have only 1 time series\n",
    "    \n",
    "        # Convert train type to number\n",
    "        target = train_to_id5(train_type)\n",
    "        \n",
    "        # Add to dataset to be fed to a machine learning algorithm\n",
    "        model_input.append(input_vector)\n",
    "        model_target.append(target)\n",
    "    \n",
    "    # Convert to a more digestable format and return the data, also makes also signals equally long\n",
    "    model_input = sequence.pad_sequences(model_input, input_length)\n",
    "    model_target = np.array(model_target)\n",
    "    return model_input, model_target\n",
    "\n",
    "\n",
    "# Load the data\n",
    "training_x, training_y = load_dataset(dataset='training')\n",
    "validate_x, validate_y = load_dataset(dataset='validate')\n",
    "\n",
    "# Transform the data / extract features\n",
    "training_x, training_y = extract_features(training_x, training_y)\n",
    "validate_x, validate_y = extract_features(validate_x, validate_y)\n",
    "\n",
    "# Build a Convolutional Neural Network\n",
    "# ------- You can change the number of filters and kernel_size here --------- #\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=4, kernel_size=5, padding='valid', input_shape=training_x.shape[1:]))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Conv1D(filters=4, kernel_size=5, padding='valid'))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=5, activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fit a model to the data. Note less epochs are needed here\n",
    "# ------- You can change the number of epochs and batch_size here ----------- #\n",
    "logger = model.fit(training_x, training_y, epochs=25, batch_size=16, validation_data=[validate_x, validate_y])\n",
    "plot_validation_history(logger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 4: It is your turn\n",
    "\n",
    "The goal now is to get the highest possible validation accuracy by tweaking the various parameters you have available. Change the amount of timesteps you include in your model, the amount of filters in the neural network or also epochs and batch_size. Keep in mind that if it seems to be running too long, you can kill it with the stop button and then undo the changes which made it take too long.\n",
    "You can also go to the \"exampls.ipynb\" notebook to get some ideas about what kind of approaches you can take. Some of these come in the form of exercises as well. The overall goal is to explore different ways to get higher validation score, so you do not need to do the exercises if you do not want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to paste code into here if you want to keep the code above clean and copy-pastable\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
